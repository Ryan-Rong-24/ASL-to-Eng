{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0274c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513cbf",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271765e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5440a",
   "metadata": {},
   "source": [
    "# 2. Keypoints using Mediapipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7329d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # drawing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604f5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False\n",
    "    results = model. process(image)                 # Make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "981047a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)    \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1ebf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OpenCV video\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(frame.shape)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc474c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec844b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d049063",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_styled_landmarks(frame,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9948a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be653557",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoints Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965e9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b951ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53330a85",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a93ab2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data_test') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = os.listdir(DATA_PATH)\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 45\n",
    "\n",
    "# Videos are going to be 45 frames in length\n",
    "sequence_length = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197193a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('MP_Data',exist_ok=True)\n",
    "for action in actions:\n",
    "    os.makedirs(os.path.join(DATA_PATH, action),exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef930155",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(1,no_sequences+1):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b12e59",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Valus for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dee707",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(1,no_sequences+1):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68c288",
   "metadata": {},
   "source": [
    "# 6.2 WLASL dataset\n",
    "https://github.com/dxli94/WLASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:\\\\Users\\\\ryanr\\\\ASLtoEng\\\\'\n",
    "MP_DATA_PATH = os.path.join(root,'MP_data_test')\n",
    "\n",
    "for label in os.listdir(MP_DATA_PATH):\n",
    "    LABEL_PATH = os.path.join(MP_DATA_PATH,label)\n",
    "    for filename in os.listdir(LABEL_PATH):\n",
    "        if filename.endswith('.mp4'):\n",
    "            print(f'Reading {filename}...')\n",
    "            \n",
    "            video_name = filename.split('.')[0]\n",
    "            VIDEO_PATH = os.path.join(LABEL_PATH,filename)\n",
    "            VIDEO_FOLDER_PATH = os.path.join(LABEL_PATH,video_name)\n",
    "            os.makedirs(VIDEO_FOLDER_PATH,exist_ok=True)\n",
    "            \n",
    "            # Capture video\n",
    "            vidcap = cv2.VideoCapture(VIDEO_PATH)\n",
    "            # Set mediapipe model \n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "                # Read feed\n",
    "                success, frame = vidcap.read()\n",
    "                \n",
    "                frame_num = 0\n",
    "                \n",
    "                while success:\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "                    \n",
    "                    # Draw detections for debugging\n",
    "#                     draw_styled_landmarks(image, results)\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "#                     cv2.waitKey(10)                    \n",
    "\n",
    "                    # Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(VIDEO_FOLDER_PATH, str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "                         \n",
    "                    success,frame = vidcap.read()\n",
    "                    frame_num += 1\n",
    "            \n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                \n",
    "                print('Converted.')                    \n",
    "                        \n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            # close the video\n",
    "            vidcap.release()\n",
    "            \n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9a1448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 00623.mp4...\n",
      "Deleting 00624.mp4...\n",
      "Deleting 00625.mp4...\n",
      "Deleting 00626.mp4...\n",
      "Deleting 00627.mp4...\n",
      "Deleting 00628.mp4...\n",
      "Deleting 00629.mp4...\n",
      "Deleting 00631.mp4...\n",
      "Deleting 00632.mp4...\n",
      "Deleting 00633.mp4...\n",
      "Deleting 00634.mp4...\n",
      "Deleting 00639.mp4...\n",
      "Deleting 01383.mp4...\n",
      "Deleting 01384.mp4...\n",
      "Deleting 01385.mp4...\n",
      "Deleting 01386.mp4...\n",
      "Deleting 01387.mp4...\n",
      "Deleting 01388.mp4...\n",
      "Deleting 01391.mp4...\n",
      "Deleting 01398.mp4...\n",
      "Deleting 65029.mp4...\n",
      "Deleting 05229.mp4...\n",
      "Deleting 05230.mp4...\n",
      "Deleting 05231.mp4...\n",
      "Deleting 05232.mp4...\n",
      "Deleting 05233.mp4...\n",
      "Deleting 05234.mp4...\n",
      "Deleting 05238.mp4...\n",
      "Deleting 05239.mp4...\n",
      "Deleting 05243.mp4...\n",
      "Deleting 65145.mp4...\n",
      "Deleting 69225.mp4...\n"
     ]
    }
   ],
   "source": [
    "# remove videos after converting\n",
    "for label in os.listdir(MP_DATA_PATH):\n",
    "    LABEL_PATH = os.path.join(MP_DATA_PATH,label)\n",
    "    for filename in os.listdir(LABEL_PATH):\n",
    "        if filename.endswith('.mp4'):\n",
    "            filepath = os.path.join(LABEL_PATH,filename)\n",
    "            print(f'Deleting {filename}...')\n",
    "            os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the data collected is correct\n",
    "for label in os.listdir(MP_DATA_PATH):\n",
    "    LABEL_PATH = os.path.join(MP_DATA_PATH,label)\n",
    "    for filename in os.listdir(LABEL_PATH):\n",
    "        if not filename.endswith('.mp4'):\n",
    "            for npfile in os.listdir(os.path.join(LABEL_PATH,filename)):\n",
    "                print(np.load(os.path.join(LABEL_PATH,filename,npfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61ec04",
   "metadata": {},
   "source": [
    "# 6.3 ASLLRP dataset \n",
    "http://www.bu.edu/asllrp/av/dai-asllvd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361c660",
   "metadata": {},
   "source": [
    "# 6.4 ASL MNIST dataset\n",
    "https://www.kaggle.com/datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b5e68",
   "metadata": {},
   "source": [
    "# 6.* Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9348f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb99787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accident': 0, 'africa': 1, 'basketball': 2}\n"
     ]
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('..')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))):\n",
    "        for frame_num in range(sequence_length):\n",
    "            if  os.path.exists(os.path.join(DATA_PATH, action, sequence, \"{}.npy\".format(frame_num))):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, sequence, \"{}.npy\".format(frame_num)))d\n",
    "            else:\n",
    "                res = np.zeros((1662,),dtype='float64')\n",
    "            # TODO: plot recognized frame\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af0cebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00623' '00624' '00625' '00626' '00627' '00628' '00629' '00631' '00632'\n",
      " '00633' '00634' '00639']\n",
      "['01383' '01384' '01385' '01386' '01387' '01388' '01391' '01398' '65029']\n",
      "['05229' '05230' '05231' '05232' '05233' '05234' '05238' '05239' '05243'\n",
      " '65145' '69225']\n"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in label_map.keys():\n",
    "    print(np.array(os.listdir(os.path.join(DATA_PATH, action))))\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            if  os.path.exists(os.path.join(DATA_PATH, action, sequence, \"{}.npy\".format(frame_num))):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, sequence, \"{}.npy\".format(frame_num)))\n",
    "            else:\n",
    "                res = np.zeros((1662,),dtype='float64')\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd8391d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bedf9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 45, 1662)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80af7dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18452d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = to_categorical(labels,num_classes=len(actions)).astype(int)\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faf134cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74e3eb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c8d32",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0e88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5345297",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0c5a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(45,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f122bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6567c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.0983 - categorical_accuracy: 0.2857\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 5.8979 - categorical_accuracy: 0.3571\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 37.7889 - categorical_accuracy: 0.4643\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 19.8545 - categorical_accuracy: 0.3571\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 15.2024 - categorical_accuracy: 0.2857\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 13.1622 - categorical_accuracy: 0.2857\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 132.1334 - categorical_accuracy: 0.3571\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 33.9515 - categorical_accuracy: 0.3214\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 19.9769 - categorical_accuracy: 0.3214\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 43.8152 - categorical_accuracy: 0.3929\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 28.9343 - categorical_accuracy: 0.3214\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 26.7480 - categorical_accuracy: 0.3214\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 1s 729ms/step - loss: 53.0077 - categorical_accuracy: 0.3929\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 34.2783 - categorical_accuracy: 0.1786\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 31.4121 - categorical_accuracy: 0.3929\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 1s 593ms/step - loss: 45.4559 - categorical_accuracy: 0.3214\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 710ms/step - loss: 25.4258 - categorical_accuracy: 0.2143\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 1s 723ms/step - loss: 37.6735 - categorical_accuracy: 0.2500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 1s 650ms/step - loss: 48.6200 - categorical_accuracy: 0.3929\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 56.3554 - categorical_accuracy: 0.3571\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 30.8509 - categorical_accuracy: 0.3929\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 54.4996 - categorical_accuracy: 0.3571\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 52.9931 - categorical_accuracy: 0.2143\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 40.0826 - categorical_accuracy: 0.2857\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 1s 759ms/step - loss: 44.9877 - categorical_accuracy: 0.4286\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 42.3295 - categorical_accuracy: 0.2857\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 35.0964 - categorical_accuracy: 0.3571\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 59.6921 - categorical_accuracy: 0.2857\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 1s 766ms/step - loss: 63.0418 - categorical_accuracy: 0.3571\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 1s 750ms/step - loss: 35.0923 - categorical_accuracy: 0.4643\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 1s 690ms/step - loss: 45.4322 - categorical_accuracy: 0.3214\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 42.6259 - categorical_accuracy: 0.2143\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 17.8855 - categorical_accuracy: 0.2143\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 1s 755ms/step - loss: 25.9602 - categorical_accuracy: 0.2857\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 1s 708ms/step - loss: 24.1161 - categorical_accuracy: 0.4286\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 50.6276 - categorical_accuracy: 0.3214\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 1s 622ms/step - loss: 51.8982 - categorical_accuracy: 0.3214\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 27.4649 - categorical_accuracy: 0.2857\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 17.4601 - categorical_accuracy: 0.3214\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 10.9016 - categorical_accuracy: 0.5357\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 18.6862 - categorical_accuracy: 0.3214\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 28.2835 - categorical_accuracy: 0.3571\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 16.7973 - categorical_accuracy: 0.4286\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 27.4169 - categorical_accuracy: 0.2857\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 1s 581ms/step - loss: 30.0383 - categorical_accuracy: 0.3214\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 1s 824ms/step - loss: 25.3184 - categorical_accuracy: 0.3929\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 29.2467 - categorical_accuracy: 0.3929\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 17.6998 - categorical_accuracy: 0.3214\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 19.8709 - categorical_accuracy: 0.2500\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 27.2822 - categorical_accuracy: 0.3214\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 20.2040 - categorical_accuracy: 0.1429\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 1s 737ms/step - loss: 20.7705 - categorical_accuracy: 0.2857\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 29.7768 - categorical_accuracy: 0.1429\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 18.6203 - categorical_accuracy: 0.4643\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 25.3679 - categorical_accuracy: 0.1429\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 17.6986 - categorical_accuracy: 0.4286\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 22.2769 - categorical_accuracy: 0.2857\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 26.2870 - categorical_accuracy: 0.2857\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 13.8731 - categorical_accuracy: 0.3929\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 23.8480 - categorical_accuracy: 0.2500\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 14.0731 - categorical_accuracy: 0.4286\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 474ms/step - loss: 73.3339 - categorical_accuracy: 0.2143\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 29.5462 - categorical_accuracy: 0.2857\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 65.5058 - categorical_accuracy: 0.2857\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 63.8797 - categorical_accuracy: 0.3214\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 83.7197 - categorical_accuracy: 0.4286\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 144.6139 - categorical_accuracy: 0.3571\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 109.8525 - categorical_accuracy: 0.3571\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 54.3432 - categorical_accuracy: 0.3929\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 74.7878 - categorical_accuracy: 0.2857\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 65.3044 - categorical_accuracy: 0.3571\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 120.4821 - categorical_accuracy: 0.3214\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 323ms/step - loss: 113.3567 - categorical_accuracy: 0.2857\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 66.7558 - categorical_accuracy: 0.4286\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 129.8331 - categorical_accuracy: 0.4286\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 104.3977 - categorical_accuracy: 0.1786\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 77.6211 - categorical_accuracy: 0.3571\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 94.6825 - categorical_accuracy: 0.2500\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 82.3800 - categorical_accuracy: 0.2857\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 41.7863 - categorical_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 107.4202 - categorical_accuracy: 0.3214\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 49.0352 - categorical_accuracy: 0.3571\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 116.5286 - categorical_accuracy: 0.4643\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 97.2541 - categorical_accuracy: 0.2857\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 106.5173 - categorical_accuracy: 0.2857\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 108.3288 - categorical_accuracy: 0.3571\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 131.2092 - categorical_accuracy: 0.2143\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 67.5096 - categorical_accuracy: 0.4286\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 84.9863 - categorical_accuracy: 0.3571\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 74.9150 - categorical_accuracy: 0.2500\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 46.8482 - categorical_accuracy: 0.3929\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 83.5775 - categorical_accuracy: 0.3571\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 114.1699 - categorical_accuracy: 0.3571\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 99.9538 - categorical_accuracy: 0.3571\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 140.6446 - categorical_accuracy: 0.2857\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 90.7435 - categorical_accuracy: 0.4286\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 68.3474 - categorical_accuracy: 0.4643\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 102.2963 - categorical_accuracy: 0.3571\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 133.3273 - categorical_accuracy: 0.3929\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 83.8778 - categorical_accuracy: 0.5000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 112.0322 - categorical_accuracy: 0.3929\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 105.2542 - categorical_accuracy: 0.3929\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 138.3869 - categorical_accuracy: 0.2500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 125.7717 - categorical_accuracy: 0.2857\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 108.1649 - categorical_accuracy: 0.3929\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 138.7815 - categorical_accuracy: 0.1429\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 127.0937 - categorical_accuracy: 0.2857\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 77.6787 - categorical_accuracy: 0.2857\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 80.2533 - categorical_accuracy: 0.4286\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 250.5346 - categorical_accuracy: 0.2857\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 237.7962 - categorical_accuracy: 0.3214\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 165.1505 - categorical_accuracy: 0.2143\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 198.6556 - categorical_accuracy: 0.3571\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 170.9863 - categorical_accuracy: 0.2857\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 173.4996 - categorical_accuracy: 0.2857\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 26.4898 - categorical_accuracy: 0.4286\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 118.2606 - categorical_accuracy: 0.3214\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 34.8507 - categorical_accuracy: 0.2500\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 30.5056 - categorical_accuracy: 0.1429\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 28.6033 - categorical_accuracy: 0.2857\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 33.2784 - categorical_accuracy: 0.3571\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 62.2317 - categorical_accuracy: 0.2500\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 25.3481 - categorical_accuracy: 0.3214\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 24.4357 - categorical_accuracy: 0.3571\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 19.5034 - categorical_accuracy: 0.3571\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 24.9342 - categorical_accuracy: 0.4643\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 476ms/step - loss: 39.5324 - categorical_accuracy: 0.3571\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 26.0254 - categorical_accuracy: 0.2500\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 24.4402 - categorical_accuracy: 0.3214\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 29.1304 - categorical_accuracy: 0.2500\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 23.9138 - categorical_accuracy: 0.3214\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 14.7498 - categorical_accuracy: 0.4643\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 30.5564 - categorical_accuracy: 0.3214\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 21.8204 - categorical_accuracy: 0.2857\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 28.8991 - categorical_accuracy: 0.4643\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 38.6205 - categorical_accuracy: 0.3571\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 33.8225 - categorical_accuracy: 0.4286\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 44.5606 - categorical_accuracy: 0.3571\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 30.9146 - categorical_accuracy: 0.2143\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 31.2415 - categorical_accuracy: 0.3214\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 66.3336 - categorical_accuracy: 0.1786\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 30.8652 - categorical_accuracy: 0.3571\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 21.9511 - categorical_accuracy: 0.3929\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 27.8553 - categorical_accuracy: 0.3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 41.6406 - categorical_accuracy: 0.3214\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 37.9077 - categorical_accuracy: 0.4643\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 19.8810 - categorical_accuracy: 0.5714\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 53.8857 - categorical_accuracy: 0.3571\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 45.1396 - categorical_accuracy: 0.4286\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 44.5343 - categorical_accuracy: 0.3214\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 55.3104 - categorical_accuracy: 0.2143\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 31.6161 - categorical_accuracy: 0.3214\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 34.1917 - categorical_accuracy: 0.3929\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 33.8929 - categorical_accuracy: 0.2500\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 37.6003 - categorical_accuracy: 0.2500\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 22.2269 - categorical_accuracy: 0.3571\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 18.1805 - categorical_accuracy: 0.3214\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 17.3295 - categorical_accuracy: 0.3571\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 50.6233 - categorical_accuracy: 0.2143\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 33.3352 - categorical_accuracy: 0.2857\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 47.9333 - categorical_accuracy: 0.3929\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 45.2869 - categorical_accuracy: 0.2857\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 80.7035 - categorical_accuracy: 0.3214\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 50.9739 - categorical_accuracy: 0.3571\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 50.7652 - categorical_accuracy: 0.2143\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 71.4051 - categorical_accuracy: 0.2500\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 90.4992 - categorical_accuracy: 0.2143\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 37.9695 - categorical_accuracy: 0.3571\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 27.4098 - categorical_accuracy: 0.6071\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 31.3118 - categorical_accuracy: 0.3214\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 55.3830 - categorical_accuracy: 0.2143\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 41.8084 - categorical_accuracy: 0.2500\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 83.0988 - categorical_accuracy: 0.2500\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 25.5737 - categorical_accuracy: 0.3929\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 23.9760 - categorical_accuracy: 0.3929\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 17.9532 - categorical_accuracy: 0.3929\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 19.3785 - categorical_accuracy: 0.2857\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 29.9777 - categorical_accuracy: 0.2857\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 25.9863 - categorical_accuracy: 0.4286\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 18.8399 - categorical_accuracy: 0.3571\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 25.9719 - categorical_accuracy: 0.3929\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 29.0368 - categorical_accuracy: 0.3571\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 33.7967 - categorical_accuracy: 0.2500\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 25.5346 - categorical_accuracy: 0.3929\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 40.5315 - categorical_accuracy: 0.3571\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 20.5632 - categorical_accuracy: 0.4643\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 28.8888 - categorical_accuracy: 0.3214\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 32.3190 - categorical_accuracy: 0.2143\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 21.9823 - categorical_accuracy: 0.2500\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 21.0943 - categorical_accuracy: 0.4643\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 25.0541 - categorical_accuracy: 0.4643\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 20.3227 - categorical_accuracy: 0.3214\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 22.9139 - categorical_accuracy: 0.3571\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 30.2337 - categorical_accuracy: 0.4286\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 13.7117 - categorical_accuracy: 0.4643\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 25.0188 - categorical_accuracy: 0.2500\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 18.8508 - categorical_accuracy: 0.2500\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 11.0612 - categorical_accuracy: 0.3214\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 16.9109 - categorical_accuracy: 0.3214\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 13.6685 - categorical_accuracy: 0.3214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x175699c8e80>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d819508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca6b74",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "814b8ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(X_test)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11ac0e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'basketball'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86561d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'basketball'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728c68d",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f3be309",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aaed4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91fea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b6708",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52446bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b939732",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a0af607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d9f3871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9980348e-01, 4.8373937e-07, 1.9605504e-04],\n",
       "       [9.9999988e-01, 7.8965980e-09, 7.5181660e-08],\n",
       "       [6.3179806e-07, 1.7409593e-09, 9.9999940e-01],\n",
       "       [4.1848630e-01, 2.3294869e-01, 3.4856501e-01]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07a6be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b493bf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26ccd589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13ef047c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [1, 0]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 1]]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a09e87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672c542",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98657014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c74474",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca55081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "africa\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "africa\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "africa\n",
      "accident\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "basketball\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "africa\n",
      "accident\n",
      "africa\n",
      "accident\n",
      "accident\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-no_sequences:]\n",
    "        \n",
    "        if len(sequence) == no_sequences:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adf8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
